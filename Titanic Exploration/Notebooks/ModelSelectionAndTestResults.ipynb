{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Pipeline: Evaluate results on validation set\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "In this section, we will use what we learned in last section to fit the best few models on the full training set and then evaluate the model on the validation set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read in data\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\r\n",
    "\r\n",
    "tr_features = pd.read_csv('../train_features.csv')\r\n",
    "tr_labels = pd.read_csv('../train_labels.csv')\r\n",
    "\r\n",
    "val_features = pd.read_csv('../val_features.csv')\r\n",
    "val_labels = pd.read_csv('../val_labels.csv')\r\n",
    "\r\n",
    "te_features = pd.read_csv('../test_features.csv')\r\n",
    "te_labels = pd.read_csv('../test_labels.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fit best models on full training set\r\n",
    "\r\n",
    "Results from last section (Fit Model):\r\n",
    "```\r\n",
    "BEST PARAMS: {'max_depth': 10, 'n_estimators': 50}\r\n",
    "\r\n",
    "0.779 (+/-0.093) for {'max_depth': 2, 'n_estimators': 5}\r\n",
    "0.792 (+/-0.131) for {'max_depth': 2, 'n_estimators': 50}\r\n",
    "0.794 (+/-0.11) for {'max_depth': 2, 'n_estimators': 100}\r\n",
    "0.811 (+/-0.058) for {'max_depth': 10, 'n_estimators': 5}\r\n",
    "rf1 --> 0.822 (+/-0.056) for {'max_depth': 10, 'n_estimators': 50}\r\n",
    "0.811 (+/-0.055) for {'max_depth': 10, 'n_estimators': 100}\r\n",
    "0.785 (+/-0.056) for {'max_depth': 20, 'n_estimators': 5}\r\n",
    "rf3 --> 0.813 (+/-0.026) for {'max_depth': 20, 'n_estimators': 50}\r\n",
    "0.807 (+/-0.019) for {'max_depth': 20, 'n_estimators': 100}\r\n",
    "0.787 (+/-0.061) for {'max_depth': None, 'n_estimators': 5}\r\n",
    "0.809 (+/-0.025) for {'max_depth': None, 'n_estimators': 50}\r\n",
    "rf2 --> 0.815 (+/-0.029) for {'max_depth': None, 'n_estimators': 100}\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#creating top 3 models based on performance of last section\r\n",
    "rf1 = RandomForestClassifier(n_estimators=50, max_depth=10)\r\n",
    "rf1.fit(tr_features, tr_labels.values.ravel())\r\n",
    "\r\n",
    "rf2 = RandomForestClassifier(n_estimators=100, max_depth=None)\r\n",
    "rf2.fit(tr_features, tr_labels.values.ravel())\r\n",
    "\r\n",
    "rf3 = RandomForestClassifier(n_estimators=50, max_depth=20)\r\n",
    "rf3.fit(tr_features, tr_labels.values.ravel())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=20, n_estimators=50)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate models on validation set\r\n",
    "\r\n",
    "**Accuracy** = #predicted correctly / total # of examples\r\n",
    "**Precision** = #predicted as surviving that actually survived / total # of predicted to survive\r\n",
    "**Recall** = #predicted as surviving that actually survived / total # that actually survived"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#loop to run the three separate models and print their results to see which model performs best on the validation set\r\n",
    "for mdl in [rf1, rf2, rf3]:\r\n",
    "    y_pred = mdl.predict(val_features)\r\n",
    "    accuracy = round(accuracy_score(val_labels, y_pred), 3)\r\n",
    "    precision = round(precision_score(val_labels, y_pred), 3)\r\n",
    "    recall = round(recall_score(val_labels, y_pred), 3)\r\n",
    "    print('MAX DEPTH: {} / # OF EST: {} -- Accuracy: {} / Precision: {} / Recall: {}'.format(mdl.max_depth,\r\n",
    "                                                                         mdl.n_estimators,\r\n",
    "                                                                         accuracy,\r\n",
    "                                                                         precision,\r\n",
    "                                                                         recall))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MAX DEPTH: 10 / # OF EST: 50 -- Accuracy: 0.816 / Precision: 0.841 / Recall: 0.697\n",
      "MAX DEPTH: None / # OF EST: 100 -- Accuracy: 0.816 / Precision: 0.812 / Recall: 0.737\n",
      "MAX DEPTH: 20 / # OF EST: 50 -- Accuracy: 0.799 / Precision: 0.794 / Recall: 0.711\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results above show that both model 1 and model 2 have the same accuracy but different precision and recall values. In this instance, we will need to decide how to prioritize precision and recall to choose what we believe to be the best model. As we provided above the definitions for **Precision** and **Recall**, we can see model 1 had a better prediction result for **Precision** but model 2 had a better prediction result for **Recall**. The variation between **Precision** for the two models is *0.029* and the variation between **Recall** is *0.040*. These two values are very similar so for the instance of this exploration, we will choose the model with the fewest parameters, which is model 1 with a max depth of 10 and 50 estimators. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate the best model on the test set\r\n",
    "\r\n",
    "Running the model on the test data set confirms there is an unbiased view of how it should perform moving forward. The final test set prediction is purely for performance evaluation to confirm the model will perform with high remarks even on data that has been unseen, this gives more confidence in the performance of the model moving forward.\r\n",
    "\r\n",
    "To perform the prediction of model 1 on the test set, we basically use the same format of code we used in the validation set, only changing the data set we are performing the prediction on (te_labels and te_features) to show as the test set. We can then make our predictions and print the results of the model to evaluate it's performance.\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "y_pred = rf1.predict(te_features)\r\n",
    "accuracy = round(accuracy_score(te_labels, y_pred), 3)\r\n",
    "precision = round(precision_score(te_labels, y_pred), 3)\r\n",
    "recall = round(recall_score(te_labels, y_pred), 3)\r\n",
    "print('MAX DEPTH: {} / # OF EST: {} -- A: {} / P: {} / R: {}'.format(rf1.max_depth,\r\n",
    "                                                                     rf1.n_estimators,\r\n",
    "                                                                     accuracy,\r\n",
    "                                                                     precision,\r\n",
    "                                                                     recall))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MAX DEPTH: 10 / # OF EST: 50 -- A: 0.792 / P: 0.741 / R: 0.662\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reviewing the results of this model we see it performed on unseen data with an **Accuracy** of *79.2%*, **Precision** of *74.1%*, and **Recall** of *66.2%*.\r\n",
    "\r\n",
    "When we first fit the model on the training set, it performed with an **Accuracy** of *82.2%*. \r\n",
    "\r\n",
    "Then when we used cross-validation to evaluate the model on the validation set, it performed with an **Accuracy** of *81.6%*, **Precision** of *84.1%*, and **Recall** of *69.7%*. \r\n",
    "\r\n",
    "We see the chosen model (RF1) from fitting on the training set, to evaluating on the validation set, to predicting on unseen data, provides an **Accuracy** difference of *3%*. This is highlighted to show that the results of a model can vary depending on the data you allow it to see. "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "493c74afe5c3389c18b31c611ca8d9adabe5d170bcbd9dca85ad5c9ec7b23f0e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}